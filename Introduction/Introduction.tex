\documentclass[../Thesis-IJspeert.tex]{subfiles}

\begin{document}

\graphicspath{ {Introduction/figs/} }
\pgfplotsset{table/search path={Introduction/data/}}

\chapter{Introduction}
\addtocontents{toc}{\vskip-6pt\par\noindent\protect\textcolor{gray75}{\protect\rule{\textwidth}{0.5pt}}\par}
\label{chap:Introduction}

\blockquote[\begin{flushright}\vspace{-0.0\baselineskip}— E. Schrödinger \end{flushright}][]{``\emph{The world is given to me only once, not one existing and one perceived. Subject and object are only one. The barrier between them cannot be said to have broken down as a result of recent experience in the physical sciences, for this barrier does not exist.}"} 

Entanglement is a counterintuitive, perhaps distressing, yet spectacular property of quantum physics that has long puzzled scientists and philosophers alike. Its implications were first discussed by Albert Einstein in 1935, in a joint paper with Boris Podolsky and Nathan Rosen \cite{PhysRev.47.777}. For a long time, the majority of physicists considered the argument to be of historical interest, yet of little relevance to modern quantum mechanics. Remarkably, more than 80 years after its publication, the article is still cited hundreds of times each year. Today we live in an era in which entanglement is no longer looked upon as a mathematical artefact nor as one of the ``\textit{ridiculous consequences}'' of quantum theory that were envisioned by Erwin Schr\"odinger in 1952 \cite{schrodinger}. In fact, current experiments exploit the properties of entanglement to resolve important questions regarding the foundations of quantum theory itself \cite{Hensen2015,Giustina2015,Shalm2015}. Our increased understanding of how we may take advantage of phenomena such as entanglement in data processing and transmission also fostered the emergence of two pioneering fields: quantum computation and quantum communication \cite{nielsen_chuang_2010}. At the heart of these disciplines lies the notion that the specific rules of quantum physics offer an alternative way of calculating and communicating, for which no classical counterpart exists.

>> section about quantum computers <<

A promising technology is that of a quantum network, which is designed for the communication of quantum information and for distributed quantum computing \cite{Kimble2008}. In particular, it enables the distribution of entanglement across the network and the teleportation of quantum states between its nodes. An important application is device-independent quantum key distribution (DI QKD) \cite{PhysRevLett.67.661,Diamanti2016}. Vital to the functionality of quantum networks is the generation of entanglement between distant nodes, which, in principle, can be achieved through the interaction of single photons and atoms \cite{Duan2001}. Interfaces for such interaction have been successfully implemented across a range of platforms, which include quantum dots \cite{Delteil2015}, defect centres in diamond \cite{Bernien2013}, trapped ions \cite{Blatt2008} and neutral atoms \cite{Wilk2007}.

>> section about dipole traps and cavities <<

The goal of this doctoral research is then to combine such atom traps with an optical cavity for the purpose of deterministic entanglement generation---a necessary step towards the realisation of a prototype quantum network based on trapped atoms in optical cavities.




\iffalse
Classical computation -- that is the use of bits with binary states to contain and process information -- is fundamentally limited.  The modelling and simulation of real world systems has been and remains fundamental to the advancement of our scientific understanding. In general, computational advancements allow our models to become more complex and more precise by handling ever increasing multiplicities of significant figures and variables.  However there is a catch when we use a classical system, with its binary states, to simulate a quantum system, with the superpositions of states that describe it.

Richard Feynman was an early proponent that this problem of simulating quantum mechanics may be better tackled with a quantum machine.  He argued that the exponential growth of the number of classical configurations required to describe the many states that a quantum system can be simultaneously in, fundamentally limits their application to such problems \cite{feynman82}.  The alternative, these quantum machines, can be considered as having quantum bits (qubits) with two states, 0 and 1 by convention, much like classical bits.  However, unlike their classical counterparts, qubits can also be in any linear superposition of these two states.  As well as the insights and advancements that could be gained from using quantum simulators to model quantum systems, this approach is also of interest beyond scientific modelling.  This exploitation of quantum nature also allows for greatly increased performance in certain tasks -- Shor's algorithm, finding the prime factors of a number, being a commonly referenced example \cite{shor94}.

The challenge remains, how to construct such a device.  Classically bits are realised using electronic circuitry, such as transistors, with high and low states.  Qubits however require an element that can be placed  in a quantum state and research is ongoing into a wide range of candidates -- including single photons \cite{kok16,holleczek15b, obrien07}, trapped ions \cite{eltony16, harty14, monz11}, Nitrogen-vacancy (NV) centres in diamond \cite{fuchs11,nizovtsev05, neumann08}, quantum dots \cite{michler17,feng03,imamoglu99} and superconducting devices \cite{wallraff04, leek10, benjamin15, wendin17} to name a few -- with each having its own advantages and disadvantages.

Consider photons, their nature is such that the processes through which they interact are relatively weak \cite{quantumOpticsAnIntroduction06}, which means the coherence time of a photon in free flight -- the time for which they retain the information mapped onto them before interactions with the environment corrupt or destroy it -- can be favourable when compared to more strongly interacting qubits such as the solid-state candidates previously mentioned.  Moreover any complete quantum information infrastructure must allow for the transfer of information between different computational nodes in a network, or even over much longer distances between different networks.  To this end the speed of photons travelling down optical fibres makes them an obvious candidate for providing these links \cite{duan01} (no one wants to be physically moving hardware around to send and receive information!).  However the weak interactions of photons is also a source of difficulty when it comes to performing operations on a qubit.  It was believed that this required the inclusion of some non-linear element, such as the (still very weak) cross-Kerr effect \cite{kok02,lin09,sheng08,clausen02}, until 2001 when Knill, Laflamme and Milburn showed that universal quantum computation could, in theory, be achieved with only linear optical elements, single-photon sources and single-photon detectors \cite{knill01}.  Their approach, relying on the near deterministic realisation of probabilistic gates by the use of entangled photon states and quantum teleportation, can be resource inefficient but the probabilistic realisation of quantum logic in linear optical quantum computing (LOQC) was soon demonstrated in 2002 \cite{pittman02}, with a probabilistic controlled-NOT operation following in 2003 \cite{obrien03}.  

The advancement of integrated photonic circuits to replace bulk optics \cite{politi08} began to address the issue of large optical apparatus being required to realise even simple operations, but any truly scalable realisation also requires an approaching deterministic source of single photons in well defined quantum states.  Sources based on spontaneous parametric down conversion (SPDC) in non-linear crystals \cite{motes13} are widely used for proof-of-principle LOQC experiments.  The crystals split an incoming `pump' photon into pairs of output photons, with the detection of one of these (the `signal') heralding the presence of the other (`idler').  This process only occurs with a very low efficiency (${\sim}10^{-9}$) and so is inherently limited in its suitability to both producing many photon states and doing so `on-demand' (though the multiplexing of many SPDC sources is one approach to mitigating this limitations \cite{barz10,ma11, spring17} and using this many-photon interference experiments have been demonstrated \cite{spring15,menssen17}).

An alternative to entirely optical quantum computing is to combine different species of qubit, utilising each for what it is best suited.  Highly controllable qubits on which to perform the operations of the quantum computer can be chosen and, to transfer information around, these computational nodes can be linked with `flying' qubits in the form of photons.  Trapped ions are currently considered the most likely candidate for these nodes, with a demonstrated average single-qubit gate fidelity of \SI{99.9999}{\percent} \cite{harty14}.  This is the basis of the Q20:20 quantum engine proposed by the Networked Quantum Information Technologies (NQIT) hub (lead by the University of Oxford and part of the UK National Quantum Technologies Programme that was announced in 2013 \cite{natEd15}) which is targeted to be a scalable network of 20 quantum processors, each containing 20 qubits.  The challenge remains however, how do you interface these photons with the qubits in the nodes in order to map information from one to the other.

This thesis is focussed on the construction and study of system that addresses a difficulty shared by both LOQC and the hybrid Q20:20 engine -- an atom-photon interface where the fine control that can be realised on atomic states is coupled to a well defined mode of the electromagnetic field.  Towards LOQC this is a single-photon source, providing unparalleled control over the emitted photon \cite{dilley12,nisbet11,barter16,kuhn02,kuhn09,kuhn10}.  A photon emission is of course one direction of the interface required by the hybrid system, the other is mapping a photonic state back onto the atom.  Conceptually this is simply a time-reversal of the emission process, however nuances in the physical realisation make it trickier to realise.\footnotemark{}
\footnotetext{For a discussion of this see \cite{dilley12b}.  In essence, the imperfect emission probability of a photon from the atom leaves some distributed population in the system at the end of any non-infinite emission process.  Perfect time-reversal then necessities that this population distribution already be present when the absorption process begins.}

This interface is realised using a high finesse cavity, strongly coupled to an atom or ion to greatly enhance their interaction with a particular optical mode.  Ions have the additional complication of interacting with the residual charges on the dielectric coatings of the cavity mirrors -- this being the subject of other work in the NQIT hub, particularly that of Matthias Keller in Sussex \cite{takahashi13, keller07}.  Moreover, although other species of qubits have also been analogously coupled to high finesse cavities, such as quantum dots \cite{hennessy07,senellart17} and NV centres \cite{riedel17,faraon12}, the inhomogeneity of these engineered emitters gives rise to a distinguishability in the photons they produce.  Neutral atoms however, do not experience an interaction with the cavity mirrors and are inherently identical, making them ideal for use in probing the possibilities and limitations of methods to manipulate coupled emitter-cavity systems.

The system discussed in this work is considered as a single-photon source. \Crefbf{chap:PrinciplesOfTheSource} provides an introduction to coupled atom-cavity systems and, specifically, how they can be utilised as a quasi-deterministic single-photon emitter.  \Crefbf{chap:PhysicalSystem} then details the realisation of this scheme.

Next the performance of the source is characterised in \crefbf{chap:CharacterisingThePhotons}, initially by considering the statistics of the single-photon stream emitted.  The indistinguishability of these photons, a key property if they are to be used to provide many-photon states for LOQC, is then measured by observing the interference of photon pairs simultaneously incident on a beam splitter (a Hong-Ou-Mandel experiment \cite{hong87}).  Further application of these photons to quantum information processing (QIP) is then presented in \crefbf{chap:MultimodeInterferometer} where it is combined with a multimode interferometer that is integrated onto a photonic chip.  A classical characterisation of the chip's unitary allows us to predict the non-classical interference of indistinguishable photon pairs provided by the source as they pass through the chip, which is then compared to the measured behaviour.

Finally we turn our attention to interesting and unexpected effects that were observed in our system.  \Crefbf{chap:CavityBirefringenceEffects} presents a novel model for handling the coupling of an atom to a cavity of non-negligible birefringence.  This model is tested by comparison to the measured behaviour of our system, and then used to infer the general implications of such birefringence -- in particular on a system such as ours in which magnetic sublevels of the atom, with their defined polarisations, are coupled by the cavity.  Lifting the degeneracy of these sublevels, in order to address them individually, necessities the atom be in an external magnetic field.  However it was found that this field also causes mixing of the hyperfine atomic structure, resulting in a breakdown of the expected hyperfine energy levels and coupling strengths.  \Crefbf{chap:NonlinearZeemanEffects} discusses how this can be modelled, before once again considering how it impacts our system.

To conclude the major results and future prospects for this work are summarised in \crefbf{chap:Conclusion}.
\fi

%\pagebreak
%\section*{notes}
%
%\subsubsection*{Moving from the classical to the quantum}
%
%Classical computation - that is the use of bits with binary states to contain and process information - is fundamentally limited. Moore' law \cite{moore75} observes that the processing power of computers approximately doubles every two years as the number of transistors (bits) that fit onto a standard CPU increases in kind.  However the cramming of ever smaller components into computers cannot continue indefinitely - indeed transistors comprised of a single atom have already been demonstrated \cite{fuechsle12} - meaning eventually increases in computational power will necessitate increased computational size.  Fundamental classical operations such as AND and OR gates inherently destroy information as it is impossible to uniquely determine the inputs from the value of the output.  This logical irreversibility results in an unavoidable dissipation of energy as heat, so cooling increasingly larger computers would eventually become unfeasible.
%
%\subsubsection*{LOQC}
%
%\begin{itemize}
%
%	\item Any \NbyN{N}{N} unitary can be realised using only mirrors, beam splitters and phase shifters \cite{reck94}. 
%
%	\item It was believed that this required the implementation of some non-linear element, \eg{} cross-Kerr nonlinearity effect \cite{lin09,sheng08,clausen02}, until Knill, Laflamme and Milburn showed that universal quantum computation could, in theory, be achieved with only linear optical elements (+ single photons sources and detectors). \cite{knill01}
%
%	\item `LOQC usually only uses mirrors, beam splitters, phase shifters and their combinations such as Mach-Zehnder interferometers with phase shifts to implement arbitrary quantum operators. If using a non-deterministic scheme, this fact also implies that LOQC could be resource-inefficient in terms of the number of optical elements and time steps needed to implement a certain quantum gate or circuit, which is a major drawback of LOQC.' - Wikipedia
%
%\end{itemize}
%
%\subsubsection*{motivating source}
%
%There is a parallel between the difficulties of scaling up $N$-photon states (tackled by approaching deterministic sources such as that constructed here) and scaling up $N$-mode quantum circuits (discussed in \cref{chap:MultimodeInterferometer} as both the conversion from bulk optics to photonic chips and from the chaining of many smaller unitaries to direct multimode coupling).
%
%Of course multimode couplings is only possible if equivalent multi-photon states can be produced.  SPDC provides short photons and is probabilistic \rarrow our source moves towards deterministic (eventually allowing $N>2$ photon states) and long photons, the later of which allows the time-resolved interaction to be studied.

\end{document}
